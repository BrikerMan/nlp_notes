{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALBERT\n",
    "[ALBERT](https://arxiv.org/pdf/1909.11942.pdf),A Lite Bert    \n",
    "参考链接：   \n",
    "https://amitness.com/2020/02/albert-visual-summary/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT 的缺点\n",
    "内存限制和通讯开销：\n",
    "- BERT非常大的模型，巨量的训练参数，想要从头开始训练，需要大量的计算资源，且计算机内存会限制模型大小\n",
    "- 一种解决方法就是分布式训练；数据被拆分到多个机器上，单独进行训练，然后再同步不同的梯度，大量参数的同步过程造成较大的通讯开销，延缓训练过程；同理 模型并行化 时，也会造成同样的瓶颈 \n",
    "\n",
    "模型退化：\n",
    "- 模型变大，并不一定会获得更好的性能，反而性能可能下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALBERT的改进\n",
    " ALBERT 本质就是对 BERT 模型压缩后的产物。\n",
    " > 模型压缩有很多手段，包括剪枝，参数共享，低秩分解，网络结构设计，知识蒸馏等\n",
    " \n",
    " ALBERT降低了模型参数量从而降低了模型的训练时间（通信开销降低），但是，**模型的预测推理时间并没有降低**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-layer parameter sharing\n",
    "- BERT-LARGE 有 24 层，而 BERT-BASE 有 12 层；随着层数增加，参数量指数级增加   \n",
    "      \n",
    "      \n",
    "- ALBERT 提出了 跨层参数共享 的概念，不是单独学习 BERT 每一层的参数，而是将每一层的参数设置为相同\n",
    "    - 可以只共享 feed-forward 层的参数，或 attention 层的参数，或整个编码层的参数\n",
    "    - 共享整层参数时，模型参数量明显下降，从 110M 下降到 31M \n",
    "    \n",
    "- 参数共享对性能的影响很小，且能够帮助模型稳定网络的参数。这点是通过L2距离与 cosine similarity 得出的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SOP 替代 NSP\n",
    "- SOP 全称为 Sentence Order Prediction，其用来取代 NSP 在 BERT 中的作用，\n",
    "    - ROBERTA 和 XLNET 模型显示 NSP 非但没有作用，反而会对模型带来一些损害。删除 NSP 任务，模型在一些列任务上的性能会提升\n",
    "    - NSP 相比 MLM 并不是一个难的任务，NSP 设置到主题预测和连贯性预测，主题预测比较容易学习，因为与 MLM 任务重合；因此就算没有学到 连贯性预测，NSP 也会得到较高的精度\n",
    "    \n",
    "      \n",
    "- SOP的方式与NSP相似，也是判断第二句话是不是第一句话的下一句，但对于负例来说，SOP并不从不相关的句子中生成，而是将原来连续的两句话翻转形成负例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorized embedding parameterization\n",
    "\n",
    "通过对Embedding 部分降维来达到降低参数的作用。\n",
    "- BERT-Base 中 Embedding 层的维度与隐层的维度一样都是768；  \n",
    "- 而对于词的分布式表示，如 Word2Vec 就多采用50或300这样的维度。\n",
    "- 通过将 Embedding 部分分解来达到降低参数量的作用，其以公式表示如下：\n",
    "\n",
    "$$O(V\\times H) \\rightarrow O(V\\times E+E\\times H)$$\n",
    "\n",
    "    V：词表大小；H：隐层维度；E：词向量维度\n",
    "\n",
    "以 BERT-Base 为例，    \n",
    "- 词表大小为3w，此时的参数量为：768 * 3w = 23040000。 \n",
    "- 如果将 Embedding 的维度改为 128，那么此时Embedding层的参数量为： 128 * 3w + 128 * 768 = 3938304。\n",
    "- Embedding参数量从原来的23M变为了现在的4M，\n",
    "- 但从整个模型来看，BERT-Base的参数量在 110M，降低19M也不能产生什么革命性的变化。\n",
    "- 可以说 Embedding 层的因式分解其实并不是降低参数量的主要手段。\n",
    "- 意忽略了Position Embedding的那部分参数量， 主要是考虑到512相对于3W显得有点微不足道。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-09T10:11:47.753579Z",
     "start_time": "2020-05-09T10:11:47.748145Z"
    }
   },
   "source": [
    "### ALBERT 的性能提升\n",
    "- 参数量比 BERT-large 小 18 倍\n",
    "- 训练比 BERT 快 1.7 倍\n",
    "- 在 GLUE, RACE 和 SQUAD 任务上获得最佳性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
